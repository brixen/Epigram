
\section{The Type Theory}
\label{sec:type-theory}

One challenge in writing this paper is to extricate our account of datatypes from what else is new in Epigram 2. In fact, we demand relatively little from the setup, so we shall start with a `vanilla' theory and add just what we need. The reader accustomed to dependent types will recognise the basis of her favourite system; for those less familiar, we try to keep the presentation self-contained.

\subsection{Base theory}

\begin{wstructure}
<- Presentation of the formalism
    <- Standard presentation
        -> No novelty here
    <- 3 judgments [equation]
        -> Context validity
        -> Typing judgements
        -> Equality judgements
\end{wstructure}

We adopt a traditional presentation for our type
theory, with three mutually defined systems of judgments:
\emph{context validity}, \emph{typing}, and \emph{equality},
with the following forms:


\[
\begin{array}{ll}
\G \vdash \Valid                & \mbox{\(\G\) is a valid context, giving types to variables} \\
\G \vdash \Bhab{\M{t}}{\M{T}}           & \mbox{term \(\M{t}\) has type \(\M{T}\) in context \(\G\)} \\
\G \vdash \Bhab{\M{s} \equiv \M{t}}{\M{T}}  & \mbox{\(\M{s}\) and \(\M{t}\) are equal at type \(\M{T}\) in context \(\G\)} \\
\end{array}
\]

\begin{wstructure}
    <- Invariants [equation]
        -> By induction on derivations
\end{wstructure}

The rules are formulated to ensure that the
following `sanity checks' hold by induction on derivations
\[
\begin{array}{l@{\;\Rightarrow\;\;}l}
\G            \vdash \Bhab{\M{t}}{\M{T}}            
    & \G \vdash \Valid\; \wedge\; \G\vdash\Type{\M{T}} \\
\G            \vdash s \equiv \Bhab{\M{t}}{\M{T}}   
    & \G \vdash \Bhab{s}{T} \;\wedge\; \G\vdash \Bhab{\M{t}}{\M{T}}
\end{array} \]
and that judgments \(\M{J}\) are preserved by well-typed instantiation.
\[
\G ; \xS ; \Delta \vdash \M{J}            \;\Rightarrow\;      
    \G \vdash \Bhab{\M{s}}{\M{S}} \;\Rightarrow\; 
          \G ; \Delta[\M{s}/\x] \vdash \M{J}[\M{s}/\x] 
\]

%We are not going to prove the validity of these invariants. They
%follow rather straightforwardly from the induction rules. For formal
%proofs, we refer the reader to standard presentations of type theory,
%such as Luo's seminal work \cite{luo:utt}.

\begin{wstructure}
    <- Judgemental equality
        <- Presentation independent of particular implementation choice
        -> Model in Agda, intensional
        -> Used in Epigram, OTT
\end{wstructure}

We specify equality as a judgment, leaving open the details of its implementation, requiring only a congruence including ordinary computation (\(\beta\)-rules), decided, e.g., by testing \(\alpha\)-equivalence of \(\beta\)-normal forms~\cite{DBLP:journals/jfp/Adams06}. Coquand and Abel feature prominently in a literature of richer equalities, involving \(\eta\)-expansion, proof-irrelevance and other attractions~\cite{DBLP:journals/scp/Coquand96,DBLP:conf/tlca/AbelCP09}. Agda and Epigram 2 support such features, Coq currently does not, but they are surplus to requirements here.

% Therefore, we are not tied to a particular implementation
%choice. In particular, our system has been modelled in Agda, which
%features an intensional equality. On the other hand, it is used in
%Epigram, whose equality has a slightly extensional
%flavor~\cite{altenkirch:ott}. We expect users of fully extensional
%systems to also find their way through this presentation.

\begin{wstructure}
<- Context validity [figure no longer]
    <- Not much to be said
\end{wstructure}

Context validity ensures that variables inhabit well-formed
sets.
\[
%% Empty context validity
\Axiom{\vdash \Valid}
\qquad
%% Extend context
\Rule{\G       \vdash \Type{\M{S}}}
     {\G ; \xS \vdash \Valid}\;\x\not\in\G
\]
%
\begin{wstructure}
<- Typing judgements [figure]
    <- Set in Set
        -> For simplicity of presentation
        -> Assume that a valid stratification can be inferred
            <- Harper-Pollack, Luo, Courant
        -> See later discussion
    <- Standard presentation of Pi and Sigma types
\end{wstructure}
%
The basic typing rules
% (Fig.~\ref{fig:typing-judgements})
for tuples and functions are also standard, save that we locally adopt
\(\Set:\Set\), putting presentation before paradox~\cite{girard:set-in-set}.
The usual remedies apply, \emph{stratifying}
\(\Set\)~\cite{harper:implicit-universe, luo:utt,
  courant:explicit-universe}.
\input{figure_typing_judgements}

\paragraph{Notation.} We subscript information needed for type synthesis
but not type checking, e.g., the domain of a \(\LAMBINDER\)-abstraction,
and suppress it informally where clear. Square brackets denote tuples,
with a LISP-like right-nesting convention: \(\sqr{a\;b}\) abbreviates
\(\pair{a}{\pair{b}{\void}{}}{}\).

%We recognise the standard presentation of $\Pi$ and $\Sigma$
%types, respectively inhabited by lambda terms and dependent
%pairs. Naturally, there are rules for function application and
%projections of $\Sigma$-types. Equal types can be substituted, thanks
%to the conversion rule.

%For the sake of presentation, we postulate a $\Set$ in $\Set$
%rule. Having this rule makes our type theory inconsistent, by Girard's
%paradox~\cite{girard:set-in-set}. However, it has been
%shown~\cite{harper:implicit-universe, luo:utt,
%  courant:explicit-universe} that a valid stratification can be
%inferred, automatically or semi-automatically. In the remaining of our
%presentation, we will assume that such a stratification exists, even
%though we will keep it implicit. We shall discuss this assumption in
%Section~\ref{sec:discussion}.

%\begin{figure}
%
%\input{figure_typing_judgements}
%
%\caption{Typing judgements}
%\label{fig:typing-judgements}
%
%\end{figure}


\begin{wstructure}
<- Judgemental equality [figure]
    <- symmetry, reflexivity, and transitivity
    <- beta-rules for lambda and pair
    <- xi-rule for functions
    -> Agnostic in the notion of equality
        <- Doesn't rely on a ``propositional'' equality
        -> Key: wide applicability of our proposal
\end{wstructure}

The judgmental equality comprises the computational rules below,
closed under reflexivity,
symmetry, transitivity and structural congruence, even under binders.
We omit the mundane rules which ensure these closure properties for
reasons of space.
\input{figure_judgemental_equality}
Given a suitable stratification of \(\Set\), the
computation rules yield a terminating evaluation procedure, ensuring
the decidability of equality and thence type checking.

%Finally, we define the rules governing judgemental equality in
%Figure~\ref{fig:judgemental-equality}. We implicitly assume that
%judgemental equality respects symmetry, reflexivity, and
%transitivity. We capture the computational behavior of the language
%through the $\beta$-rules for function application and pairs. Finally,
%we implicitly assume that it respects purely syntactic and structural
%equality. This includes equality under lambda ($\xi$-rule).

%Crucially, being judgemental, this presentation is agnostic in the
%notion of equality actually implemented. Indeed, our typing and
%equality judgements do not rely on a ``propositional'' equality. This
%freedom is a key point in favour of the wide applicability of our
%proposal. This judgemental presentation must be read as a
%\emph{specification}: our proposal works with any propositional
%equality satisfying this specification. Moreover, our lightweight
%requirements do not endanger decidability of equality-checking.
%Obviously, when implementing our technology in an existing
%type-theory, some opportunities arise. We will present some of them
%along the course of the paper.


%\begin{figure}

%\input{figure_judgemental_equality}
%
%\caption{Judgemental equality}
%\label{fig:judgemental-equality}
%
%\end{figure}



\begin{wstructure}
!!! Need Help !!!
<- Meta-theoretical properties
    <- Assuming a stratified discipline
    <> The point here is to reassert that dependent types are not evil, 
       there is no non-terminating type-checker, or such horrible lies <>
    -> Strongly normalising
        -> Every program terminates
    -> Type-checking terminates
    ???
\end{wstructure}


%This completes our presentation of the type theory. Assuming a
%stratified discipline of universe, the system we have described enjoy
%some very strong meta-theoretical properties. Unlike simply typed
%languages, such as Haskell, dependently-typed systems are
%\emph{strongly normalising}: every program that type-checks
%terminates. Moreover, type-checking is decidable and can therefore be
%implemented by a terminating algorithm.
%\note{Need some care here. Expansion would be good too. I wanted to
%  carry the intuition that we are not the bad guys with a
%  non-terminating type-checker.}

\subsection{Finite enumerations of tags}
\label{sec:finite-sets}

\begin{wstructure}
<- Motivation
    <- Finite sets could be encoded with Unit and Bool
        /> Hinder the ability to name things
    <- W-types considered harmful?
        ???
    -> For convenience
        <- Named elements
        <- Referring by name instead of code
        -> Types as coding presentation
            /> Also as coding representation!
\end{wstructure}

It is time for our first example of a \emph{universe}. You might
want to offer a choice of named constructors in your datatypes, we shall
equip you with sets of tags to choose from. Our plan
is to implement (by extending the theory, or by encoding) the signature
\[
  \Type{\EnumU}\qquad \Type{\EnumT{(\Bhab{\M{E}}{\EnumU})}}
\]
where some value \(E:\EnumU\) in the `enumeration universe' describes
a type of tag choices \(\EnumT{E}\). We shall need
some tags---valid identifiers, marked to indicate that
they are data, not variables scoped and substitutable---so we hardwire these
rules:
\[
%% UId
\Rule{\Gamma \vdash \Valid}
     {\Gamma \vdash \Type{\UId}}
\qquad
%% Tag
\Rule{\Gamma \vdash \Valid}
     {\Gamma \vdash \Bhab{\Tag{\V{s}}}{\UId}}\;\V{s}\: \mbox{a valid identifier}
\]
Let us describe enumerations as lists of tags, with signature:
\[
\Bhab{\NilE}{\EnumU}\qquad
\Bhab{\ConsE{(\Bhab{\M{t}}{\UId})}{(\Bhab{\M{E}}{\EnumU})}}{\EnumU}
\]
What are the \emph{values} in \(\EnumT{E}\)? Formally, we represent
the choice of a tag as a numerical index into \(E\), via new rules:
\[
%% Ze
\Rule{\Gamma \vdash \Valid}
     {\Gamma \vdash \Bhab{\Ze}{\EnumT{(\ConsE{\M{t}}{\M{E}})}}} 
\qquad
%% Su
\Rule{\Gamma \vdash \Bhab{\M{n}}{\EnumT{\M{E}}}}
     {\Gamma \vdash \Bhab{\Su{\M{n}}}{\EnumT{(\ConsE{\M{t}}{\M{E}})}}}
\]
However, we expect that in practice, you might rather refer to these
values \emph{by tag}, and we shall ensure that this is possible in due course.

%As a motivating example, we are now going to extend the type theory
%with a notion of finite set. One could argue that there is no need for
%such an extension: finite sets, just as any data-structure, can be
%encoded inside the type theory. A well-known example of such encoding
%is the Church encoding of natural numbers, which is isomorphic to
%finite sets. \note{Shall we talk about W-types encoding?}

%However, using encodings is impractical. In the case of finite sets,
%for instance, we would like to name the elements of the sets. Then, we
%need to be able to manipulate these elements by their name, instead of
%their encoding. While we are able to give names to encodings, it is
%extremely tedious to map the encodings back to a name. Whereas these
%objects have a structure, the structure is lost during the encoding,
%when they become anonymous inhabitants of a $\Pi$ or $\Sigma$-type.

%In the simply-typed world, we are used to see types as a coding
%presentation -- presentation of invariants, presentation of
%properties. In the dependently-typed world, we also learn to use types
%as a coding representation: finite sets being good citizens, they
%ought to be democratically represented at the type level. As we will
%see, this gives us the ability to name and manipulate them (this is
%were the democracy analogy goes crazy, I think).
%\note{Did I got the coding presentation vs. coding representation
%  story right? No.}

\begin{wstructure}
<- Implementation [figure]      
    <- Tags
        -> Purely informational token
    <- EnumU
        -> Universe of finite sets
    <- EnumT e
        -> Elements of finite set e
\end{wstructure}

%The specification of finite sets is presented in
%Figure~\ref{fig:typing-finite-set}. It is composed of three
%components. First, we define tags as inhabitants of the $\UId$ type. A
%tag is solely an informative token, used for diagnostic
%purposes. Finite sets inhabits the $\EnumU$ type. Unfolding the
%definition, we get that a finite set is a list of tags. Finally,
%elements of a finite set $\V{u}$ belong to the corresponding $\EnumT{\V{u}}$
%type. Intuitively, it corresponds to an index -- a number -- pointing
%to an element of $\V{u}$.

%\begin{figure}

%\input{figure_finite_sets}

%\caption{Typing rules for finite sets}
%\label{fig:typing-finite-set}

%\end{figure}


\begin{wstructure}
<- Equipment
    <- \spi operator
        <- Equivalent of Pi on finite sets
        <- First argument: (finite) domain
        <- Second argument: for each element of the domain, a co-domain
        -> Inhabitant of \spi: right-nested tuple of solutions
            <- Skip code for space reasons
    <- switch operator
        <- case analyses over x
        <- index into the \spi tuple to retrieve the corresponding result
\end{wstructure}

Enumerations come with further machinery. Each \(\EnumT{E}\) needs an
eliminator, allowing us to branch according to a tag choice. Formally,
whenever we need such new computational facilities, we add primitive
operators to the type theory and extend the judgmental equality with
their computational behavior. However, for compactness and readability, we
shall write these operators as functional programs (much as we
model them in Agda).

We first define the `small product' $\spi{\!}{\!}$ operator:
\[\stk{
%% spi
\spi{}{}: \PITEL{\V{E}}{\EnumU}\PITEL{\V{P}}{\EnumT{\V{E}} \To \Set} \To \Set \\
\begin{array}{@{}l@{\:}l@{\:\mapsto\:\:}l}
\spi{\NilE}{& \V{P}}        & \Unit \\
\spi{(\ConsE{\V{t}}{\V{E}})}{& \V{P}} & \TIMES{\V{P}\: \Ze}{\spi{\V{E}}{(\LAM{\V{x}} \V{P}\: (\Su{\V{x}}))}}
\end{array}
}\]
This builds a right-nested
tuple type, demanding an object of $P\:i$ for each $i$ in the given finite
domain. We can see these tuples as `jump tables' tabulating dependently
typed functions from the domain. We give this functional
interpretation---the eliminator we need---by the
$\F{switch}$ operator, which, unsurprisingly, iterates projection:
\[\stk{
%% switch
\begin{array}{@{}ll}
\F{switch} :& \PITEL{\V{E}}{\EnumU}
               \PITEL{\V{P}}{\EnumT{\V{E}} \To \Set} \To\\
             & \spi{\V{E}}{\V{P}} \To
               \PITEL{\V{x}}{\EnumT{\V{E}}} \To \V{P}~\x
\end{array} \\
\begin{array}{@{}l@{\:\mapsto\:\:}l}
\switch{(\ConsE{\V{t}}{\V{E}})}{\V{P}}{\V{b}}{\Ze}      & \fst{\V{b}} \\
\switch{(\ConsE{\V{t}}{\V{E}})}{\V{P}}{\V{b}}{(\Su{\V{x}})} & \switch{\V{E}}{(\LAM{\V{x}} \V{P}
  (\Su{\V{x}}))}{(\snd{\V{b}})}{\V{x}}
\end{array}
}\]

%Again, there is a clear equivalent in the full-$\Set$ world: function
%application. The operational behaviour of $\F{switch}$ is
%straightforward: $\V{x}$ is peeled off as we move deeper inside the nested
%tuple $\V{b}$. When $\V{x}$ equals $\Ze$, we simply return the value we are
%pointing to.

\begin{wstructure}
<- Equivalent to having a function space over finite sets
    /> Made non-obvious by low-level encodings
        <- General issue with codes
             -> Need to provide an attractive presentation to the user
    -> Types seem to obfuscate our reading
        <- Provide ``too much'' information
        /> False impression: information is actually waiting to be used more widely
        -> See next Section
\end{wstructure}

The $\spi{\!}{\!}$ and $\F{switch}$ operators deliver dependent
elimination for finite enumerations, but are rather awkward to
use directly. We do not write the range for a \(\LAMBINDER\)-abstraction, so
it is galling to supply \(\V{P}\) for functions defined by $\F{switch}$.
Let us therefore find a way to recover the tedious details of the
encoding from types.

%, they also
%come with a notion of finite function space. However, we had to
%extract that intuition from the type, by a careful reading. This seems
%to contradict our argument in favour of types for coding
%representation. Here, we are overflown by low-level, very precise type
%information.

%However, our situation is significantly different from the one we
%faced with encoded data: while we were suffering from a crucial lack
%of information, we are now facing too much information, hence losing
%focus. This is a general issue with the usage of codes, as they convey
%much more information than what the developer is willing to see. 

%As we will see in the following section, there exists a cure to this
%problem. In a nutshell, instead of being overflown by typing
%information, we will put it at work, automatically. The consequence is
%that, in such system, working with codes is \emph{practical}: one
%should not be worried by information overload, but how to use it as
%much as possible. Therefore, we should not be afraid of using codes for
%practical purposes.


\subsection{Type propagation}
\label{sec:type-propagation}

\begin{wstructure}
<- Bidirectional type-checking [ref. Turner,Pierce]
    -> Separating type-checking from type synthesis
    <- Type checking: push terms into types
        <- Example: |Pi S T :>: \ x . t| allows us to drop annotation on lambda
    <- Type synthesis: pull types out of terms
        <- Example: |x : S l- x :<: S| gives us the type of x
\end{wstructure}

Our proposal to tame the problem of codes is deeply rooted in the
bidirectional presentation of type-checking from Pierce and
Turner~\cite{pierce:bidirectional-tc}. In this formalism,
type-checking is divided into two complementary phases. In the
\emph{type inference} phase, types are \emph{pulled} out of terms. A
typical example is a variable in the context:

\[
\Rule{\G ; \xS ; \Delta \vdash \Valid}
     {\G ; \xS ; \Delta \vdash \Bhab{\V{x}}{\V{S}}}
\]

Because the variable and its type are stored in the context, whenever
this variable is used, we can pull its type out of it.

On the other hand, in the \emph{type checking} phase, terms are
\emph{pushed} into types. We are handed a type together with a term,
our task consists in checking that the term is indeed of this
type. Doing so, we are allowed, and actually encouraged, to use the
information provided by the type. Therefore, we can relax our
requirements on the term. A typical example is the lambda abstraction:

\[
\Rule{\G       \vdash \Type{\V{S}} \quad
      \G ; \xS \vdash \Bhab{\V{t}}{\V{T}}}
     {\G \vdash \Bhab{\PLAM{\x}{\V{S}} \V{t}}{\PIS{\xS} \V{T}}}
\]

Here, we are forced to annotate lambda abstractions by the type of the
domain. In a bidirectional setting, the domain is provided by the
$\Pi$ type we are pushing the abstraction into. Hence we do not need
the annotation.

\begin{wstructure}
<- Formalisation: type propagation
    <- Motivation
        -> High-level syntax
            -> exprIn: types are pushed in
                <- Subject to type *checking*
            -> exprEx: types are pulled from
                <- Subject to type *synthesis*
        -> Translated into our low-level type theory
        -> Presented as judgements
    -> Presentation mirrors typing rule of [figure] 
        -> Ignore identical judgements
\end{wstructure}

We adapt and formalise this \emph{type propagation} mechanism to our
richer type theory. First of all, we define a high-level syntax,
composed of \emph{expressions}. Expressions are divided into two
syntactic categories: $\exprIn$ into which types are pushed in, and
$\exprEx$ from which types are pulled out. Following the bidirectional
presentation, $\exprIn$ are subject to type \emph{checking}, while
$\exprEx$ are subject to type \emph{synthesis}.
\note{Why don't we need a context validity judgement, again?}

The purpose of type propagation is to \emph{elaborate} a high-level
expression into a term of the type theory, together with its
type. This presentation builds on the experience gained in
implementing Epigram 1~\cite{mcbride.mckinna:view-from-the-left}. Being
entirely syntax driven, we present our formalism as a system of
syntax-directed inference rules. As the presentation largely mirrors
the inference rules of the type theory, we will ignore the judgements
that are identical. We refer our reader to the associated technical
report~\cite{chapman:desc-tech-report} for the complete system of
rules.

\begin{wstructure}
<- Type synthesis [figure]
    <- Pull a type out of an exprEx
    <- Result in a full term, together with its type
    -> Do *not* need to specify types
        -> Extracting a term from the context
        -> Function application
        -> Projections
\end{wstructure}

The type synthesis rules are presented
Figure~\ref{fig:type-synthesis}. As illustrated by the judgement form,
a type synthesis problem is provided an expression $\exprEx$ in a
context, which is elaborated into a low-level term together with its
type. These terms act as \emph{sources} for the type propagation
machinery: they make types out of expressions. We have already studied
the case of a variable in context. This also include function
application as well as projections. Further down the line, these
inferred types might propagate to other terms, thanks to type
checking.

\begin{figure}
\input{figure_type_synthesis}
\caption{Type synthesis}
\label{fig:type-synthesis}
\end{figure}



\begin{wstructure}
<- Type checking [figure]
    <- Push a type in an exprIn
    <- Result in a full term
    -> *Use* the type to build the term!
        -> Domain and co-domain propagation for Pi and Sigma
        -> Translation of 'tags into EnumTs
        -> Translation of ['tags ...] into EnumUs
        -> Finite function space into switch
\end{wstructure}

Dually to type synthesis, type checking judgements
(Fig.~\ref{fig:type-checking}) act as \emph{sinks}. From an expression
$\exprIn$ and a type propagated down to it, they elaborate a low-level
term. The strength of this approach comes from the availability of the
type to drive the elaboration of expressions. Its benefit on lambda
abstraction and pairs is well-known. Let us explore the consequences
on our coding scheme for finite sets.

A first consequence is that we can actually denote elements of finite
sets \emph{by their name}. Indeed, such expression will be pushed into
a $\EnumT{\V{u}}$, where $\Bhab{\V{u}}{\EnumU}$ contains all the names
populating the finite set. It is therefore easy to elaborate a name
into the corresponding index.

A second consequence concerns the definition of a finite set. We can
abstract away the details of code and use a familiar syntax
instead. Hence, we use the syntax $[t_1 \ldots t_n]$ to denote the
finite sets composed of the named elements $t_1$, \ldots, $t_n$. This
syntax is reminiscent to Lisp notation for lists. The translation to
elements of $\EnumU$ is straightforward.

Finally, and more importantly, we are able to present the developer an
abstraction of \emph{finite} function space. This works as follow. When
an expression $\V{t}$ is pushed into a function which domain is a finite
set, we instead elaborate it as a term in a small $\pi$-type. That is,
a finite function. To realise our promise of a big $\Pi$-type, we
elaborate a call to the $\F{switch}$ eliminator.

The abstraction we have developed in this section is entirely
transparent to the user. In the case of finite function space, the
developer never has to explicitly \emph{call} the $\F{switch}$
eliminator. For diagnosis purposes, a similar, reverse mechanism turns
low-level terms into high-level expressions. Hence, this system, whose
complete presentation is out of the scope of this paper, seals the
low-level type theory behind the high-level expressions.

\begin{figure}
\input{figure_type_checking}
\caption{Type checking}
\label{fig:type-checking}
\end{figure}

\begin{wstructure}
<- Summary
    -> Not a novel technique [refs?]
        /> Used as a boilerplate scrapper
    -> Make dealing with codes *practical*
        <- Example: Finite sets/finite function space
        -> We should not restrain our self in using codes
            <- We know how to present them to the user
-> Will extend this machinery in further sections
\end{wstructure}

In this section, we have developed a type propagation system based on
bidirectional type-checking. Using bidirectional type-checking as a
boilerplate scrapper is a well-known
technique~\cite{pierce:bidirectional-tc,
  xi:bidirectional-tc-bound-array, chlipala:strict-bidirectional-tc}
\note{Everybody ok with the citations?}. In our case, we have shown
how to instrument bidirectionality to rationalise the expressivity of
dependent types. We have illustrated our approach with finite sets. We
have abstracted away the low-level presentation of finite sets,
offering a convenient syntax instead.

This example teaches us that we should not be afraid of codes, as soon
as type information is available. We have shown how to rationalise
this information in a formal presentation. Hence, we have shown that
programming with such objects is practical. As we introduce more codes
in our theory, we will show how to extend the framework we have
developed so far.
