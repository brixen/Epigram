\section{A universe of inductive datatypes}
\label{sec:universe-desc}

\begin{wstructure}
<- Why starting with simple datatypes
    <- For pedagogical purposes
        <- datatypes as we know them every day
        /> Target dependent types
    -> Cut down version of Induction Recursion
        -> Presentation evolves independently as we go to dependent types
\end{wstructure}

In this section, we describe an implementation of inductive types, as
we know them in ML-like languages. By working with familiar datatypes,
we hope to focus on the delivery mechanism, warming up gently to the
indexed datatypes we really want.  % Our encoding is inspired by
Dybjer and Setzer's closed formulation of
induction-recursion~\cite{dybjer:axiom-ir}, but without the
`-recursion'.  An impredicative Church-style encoding of datatypes is
not adequate for dependently typed programming, as although such
encodings present data as non-dependent eliminators, they do not
support dependent \emph{induction}~\cite{geuvers:induction-not-derivable}.
Whilst the \(\LAMBINDER\)-calculus captures all that data can \emph{do},
it cannot ultimately delimit all that data can \emph{be}.

\subsection{The power of $\Sigma$}

\begin{wstructure}
<- The duality of Sigma
    <- Sigma generalises sum over arbitrary arities
        -> \Sigma A B == \Sigma_{x : A} B x
    <- Sigma generalises product to have a dependant second component
        -> \Sigma A B == (x : A) \times (B x)
\end{wstructure}

In dependently typed languages, $\Sigma$-types can be interpreted as
two different generalisations. This duality is reflected in the
notation we can find in the literature. The notation
$\blue{\Sigma}_{\x : \M{A}} (\M{B}\: \x)$ stresses that
$\Sigma$-types are `dependent sums', generalising of sums over
arbitrary arities, where simply typed languages have finite sums.

On the other hand, our choice of notation $\SIGMA{\x}{\M{A}}
(\M{B}\:\x)$ emphasises that $\Sigma$-types generalise products,
with the type of the second component depending on the value of the
first, where simply typed languages do not express such relative validity.

\begin{wstructure}
<- datatypes in the simply-typed world
    -> "sums-of-product"
        <- Sum of constructors
        <- Product of arguments
<- datatypes in the dependently-typed world
    -> "sigmas-of-sigmas"
    /> Need ability to manipulate these sigmas
        -> Define a Code for datatypes
        -> Together with a sigma-based Interpretation
\end{wstructure}

In ML-like languages, datatypes are presented as a
\emph{sum-of-products}. A datatype is defined by a finite sum of
constructors, each carrying a product of arguments. To embrace
these datatypes, we have to capture this grammar.
With dependent types, the notion of sum-of-products translates into
\emph{sigmas-of-sigmas}.


\subsection{A universe of datatype descriptions}
\label{sec:desc-universe}

\begin{wstructure}
<- Introduction to Universe construction
    <- Define a Code
        -> Name objects
    <- Define an Interpretation of codes into the type theory
        -> Give a semantics to objects
    -> Ability to manipulate code
    -> Ability to compute with these objects
\end{wstructure}

While sigmas-of-sigmas can give a \emph{semantics} for the
sum-of-products structure in each node of the tree-like values in a
datatype, we need to account somehow for the recursive structure which
ties these nodes together. Not for the first time, we do this by
constructing a \emph{universe}~\cite{martin-lof:itt}. Universes
are ubiquitous in dependently typed
programming~\cite{benke:universe-generic-prog, oury:power-of-pi},
but here we seek to exploit them as the foundation of our notion
of datatypes.

%The key idea behind universe construction is our ability to make names
%by defining new types. These names are called \emph{codes}. By
%defining a set of codes, we define the syntax of a language. To give
%it a semantics, we \emph{interpret} these codes back into the type
%theory. Hence, codes act as labels, while the type theory provides the
%computational machinery. Codes being mere labels, we can inspect them,
%hence regaining structural information. We can also compute over them:
%deriving new codes from others, or even functions on them.

\begin{wstructure}
<- Justification of the code 
    <- [both figures]: cannot be read separately
    <- Mimic the standard grammar of datatypes description
        -> Just as we already know it
        <- '\Sigma for making sigmas-of-sigmas
        <- 'indx for exhibiting the functoriality
            -> For recursive arguments
        <- '1 for end of description
\end{wstructure}

To add inductive types to our type theory, we build a universe of
datatype \emph{descriptions} by implementing the signature below, with
codes mimicking the grammar of datatype declarations. We
can read a description $\V{D}:\Desc$ as an \emph{endofunctor}
on $\Set$, with $\descop{\V{D}}{\!}$ its action on an object, \(\V{X}\), soon
to be instantiated recursively.
%
\[\stk{
\begin{array}{l}
\Desc : \Set\\
\DUnit : \Desc\\
\DSigma{(\Bhab{\V{S}}{\Set})}{(\Bhab{\V{D}}{\V{S} \To \Desc})} : \Desc\\
\DIndx{(\Bhab{\V{D}}{\Desc})} : \Desc \\
\end{array}
\smallskip\\
\begin{array}{l@{\V{X}\:\mapsto\:}l}     
\multicolumn{2}{l}{\descop{\_\:}{} : \Desc \To \Set \To \Set} \\
 \descop{\DUnit}{} &  \Unit \\
 \descop{\DSigma{\V{S}}{\V{D}}}{} &
     \SIGMAS{\Bhab{\V{s}}{\V{S}}}{\descop{\V{D}\,\V{s}}{\!\V{X}}}  \\
\descop{\DIndx{\V{D}}}{}  &  \TIMES{\V{X}}{\descop{\V{D}}{\V{X}}}
\end{array}
}\]
%
Descriptions are sequential structures, terminated by $\DUnit$, indicating
the empty tuple. To build
sigmas-of-sigmas, we define a $\DSigma{\!}{}$ code, interpreted as a
$\Sigma$-type. To request a recursive component, we have $\DIndx{\V{D}}$,
where \(\V{D}\) descibes the rest of the node.

To give some intuition on this universe, we now turn to some
examples. For obvious pedagogical reasons, we will manually build the
descriptions. However, it should be clear that, in practice, these
definitions are automatically computed from an Agda-like $\data$
definition.

\subsection{Examples}
\label{sec:desc-examples}

\begin{wstructure}
<- Nat
    <- Sum of zero, suc
    <- zero case: done
    <- suc case: leave open and done
    -> NatD Z = 1 + Z
\end{wstructure}

Our first example is the natural numbers, or rather its pattern
functor. Our code is presented in the high-level expression language
of Section~\ref{sec:type-propagation}. The translation back to the raw
terms should not pose any difficulty. The code is the following:
%
\[\stk{
\NatD : \Desc \\
\NatD \mapsto \DSigma{(\EnumT{[ \NatZero, \NatSuc{\!} ]})}
                     {[ \DUnit \quad (\DIndx{\DUnit}) ]}
}\]
%
Let us explain its construction. First, we use $\DSigma{\!}{\!}$ to
give a choice between the $\NatZero$ and $\NatSuc{}$ constructors. In
the $\NatZero$ case, we reach the end of the description. In the
$\NatSuc{\!}$ case, we attach a recursive argument and close the
description. In a more synthetic notation, we have described the
following functor:
%
\[    \NatD\: \V{Z} \mapsto \Unit \mathop{\D{+}} \V{Z}    \]

\begin{wstructure}
<- List
    <- Sum of nil, cons
    <- nil case: done
    <- cons case: product of X with leave open and done
    -> ListD X Z = 1 + X * Z
\end{wstructure}

With a small change to this definition, we obtain the pattern functor
for lists:
%
\[\stk{
\ListD : \Set \To \Desc \\
\ListD \: \V{X} \mapsto \DSigma{(\EnumT{[ \ListNil, \ListCons{\!}{\!} ]})}
                           {[ \DUnit \quad (\DSigma{\V{X}}{\LAM{\_} \DIndx{\DUnit}}) ]}
}\]
%
The $\NatSuc{\!}$ constructor is turned into a proper
$\ListCons{\!}{\!}$, taking an argument in $\V{X}$ followed by a
recursive argument. This code describes the following functor:
%
\[    \ListD\: \V{X}\: \V{Z} \mapsto \Unit \mathop{\D{+}} \V{X} \D{\ensuremath{\times}} \V{Z}     \]

\begin{wstructure}
<- Tree
    <- sum of leaf, node
    <- leaf case: done
    <- node case: product of X with two leave open and done
    -> TreeD X Z = 1 + X * Z * Z
\end{wstructure}

Finally, we are not limited to one recursive argument. This is
demonstrated by our description of binary trees:
%
\[\stk{
\TreeD : \Set \To \Desc \\
\begin{array}{@{}ll}
\TreeD \: \V{X} \mapsto \DSigma{ & (\EnumT{[ \TreeLeaf, \TreeNode ]}) \\}
                           { & [ \DUnit \quad (\DSigma{\V{X}}{\LAM{\_} \DIndx{(\DIndx{\DUnit})}}) ]}
\end{array}
}\]
%
Again, we are one evolutionary step away from $\ListD$. However,
instead of a single call to the induction code, we call it twice. The
interpretation of this code corresponds to the following functor:
%
\[    \TreeD\: \V{X}\: \V{Z} \mapsto \Unit \mathop{\D{+}} \V{X} \Prod \V{Z} \Prod \V{Z}     \]


\begin{wstructure}
<- Tagged description
    <- Form TDesc = List (UId x Desc) [equation]
    <- Follow usual sums-of-product presentation of datatype
        <- Finite set of constructors
        <- Then whatever you want
    -> Any Desc datatype can be turned into this form
        -> No loss of expressive power
        /> Guarantee a ``constructor form''
\end{wstructure}

From the examples above, we observe that datatypes are defined by a
$\DSigma{\!}{\!}$ which first argument is a finite set of
constructors. The descriptions fitting into this pattern are called
\emph{tagged} description. Formally, we have:
%
\[
 \TagDesc \mapsto \SIGMA{\V{E}}{\EnumU} (\spi{\V{E}}{(\LAM{\_} \Desc)})
\]
%
This is a clear reminiscence of the sum-of-products style,
characterized by a finite sum of constructors. Every description can
be expressed in this style, using a singleton constructor if
necessary. Conversely, a tagged description can be turned into a
full-blown description. We denote $\toDesc{\V{TD}}$ the description
computed from the tagged description $\V{TD}$.

\begin{wstructure}
<- Fictive object [figure 'data Desc']
    -> Must be read as a type signature
    -> See further for its actual implementation
        <- Subject to our levitation exercise
\end{wstructure}

For convenience, we have taken for granted the existence of $\Desc$,
as presented in Figure~\ref{fig:desc_universe}. Although it makes no
conceptual difference, this code should rather be read as a
specification. We promise the existence of such objects, satisfying
these typing rules. It will be the subject of
Section~\ref{sec:desc-levitate} to actually implement this
specification.

\subsection{Fixpoint}
\label{sec:desc-fix-point}

\begin{wstructure}
<- Build the fixpoint of functors
    <- See examples: need to build their initial algebra
    -> Extend the type theory with Mu/Con [figure]
        <- Straightforward definition of a fixpoint
            <- Interpret D with (Mu D) as sub-objects
\end{wstructure}


So far, we have used our universe of descriptions to build pattern
functors. Being polynomial functors, they all admit a fixpoint. We
construct the fixpoint by \emph{tying the knot} of the
interpretation: the holes left open by the functor are filled by its
own recursive definition:
%
\[
\Rule{\Gamma \vdash \Bhab{\V{D}}{\Desc}}
     {\Gamma \vdash \Bhab{\Mu{\V{D}}}{\Set}} \qquad
\Rule{\Gamma \vdash \Bhab{\V{D}}{\Desc} \quad 
      \Gamma \vdash \Bhab{\V{x}}{\descop{\V{D}}{(\Mu{\V{D}})}}}
     {\Gamma \vdash \Bhab{\Con{\V{x}}}{\Mu{\V{D}}}}
\]

\begin{wstructure}
<- Elimination on Mu
    <- We are used to foldD : \forall X. (desc D X -> X) -> mu D -> X
        /> Not dependent
        -> Cannot express some (which one again?) properties
    -> Develop a dependent induction
        <- Everywhere/All
        <- Induction
    -> *Generic*
    ???
\end{wstructure}

Together with the type formers defined above, we would legibly expect
an elimination principle. Following a categorical intuition, we could
be tempted to provide it as a catamorphism:
%
\[
\F{cata} : \PITEL{\V{D}}{\Desc}
           \PI{\V{T}}{\Set}
           (\descop{\V{D}}{\V{T}} \To \V{T}) \To 
           \Mu{\V{D}} \To \V{T} 
\]
%
Whereas this definition is amply sufficient in the simply-typed world,
it comes short of its promises in the dependently-typed realm. Indeed,
this type is not dependent, being restricted to a notion of
\emph{iteration}. As this operator will be the corner stone of
inductive definitions, we are better off sharpening it before use.
Following \citet{benke:universe-generic-prog}, we adopt the following
definition:
%
\[\stk{
\begin{array}{lcl}
\F{induction} & : & \PITEL{\V{D}}{\Desc}
                    \PI{\V{P}}{\Mu{\V{D}} \To \Set}         \\
              &   & (\PI{\V{xs}}{\descop{\V{D}}{(\Mu{\V{D}})}}              \\
              &   & \ \All{\V{D}}{(\Mu{\V{D}})}{\V{P}}{\V{xs}} \To \V{P} (\Con{\V{xs}})) \To \\
              &   & \PI{\V{x}}{\Mu{\V{D}}} \V{P} \V{x} 
\end{array} \\
\F{induction}\: \V{D}\: \V{P}\: \V{ms}\: (\Con{\V{xs}}) =  \\
\qquad
    \V{ms}\: \V{xs}\: (\all{D}{\Mu{D}}{P}
                           {(\LAM{\V{x}} \F{induction}\: \V{D}\: \V{P}\: \V{ms}\: \V{x})}
                           {\V{xs}})
}\]
%
Where, intuitively, $\All{\V{D}}{\V{X}}{\V{P}}{\V{x}}$ states that
$\Bhab{\V{P}}{\V{X} \To \Set}$ holds everywhere in $\V{x}$. Its
definition is presented in Figure~\ref{fig:all-predicates}. Note that
this induction principle is the first manifestation of a generic
operation over descriptions. Any datatype we define automatically
comes with an induction principle.


\begin{figure*}

\[
\begin{array}{ll}
%%
\stk{
\begin{array}{@{}ll}
\All{\!}{\!}{\!}{\!} : & \PITEL{\V{D}}{\Desc}
                         \PITEL{\V{X}}{\Set}
                         \PITEL{\V{P}}{\V{X} \To \Set} \\
                       & \PI{\V{xs}}{\descop{\V{D}}{\V{X}}} 
                         \Set 
\end{array} \\
\begin{array}{@{}l@{}l@{\:=\:\:}l}
\All{\DUnit}{& \V{X}}{\V{P}}{\Void} &
    \Unit \\
\All{(\DSigma{\V{S}}{\V{T}})}{& \V{X}}{\V{P}}{\pair{\V{a}}{\V{b}}{}} &
    \All{(\V{T}\: \V{a})}{\V{X}}{\V{P}}{\V{b}} \\
\All{(\DIndx{\V{D}})}{& \V{X}}{\V{P}}{\pair{\V{a}}{\V{b}}{}} &
    \TIMES{\V{P}\: \V{a}}{\All{\V{D}}{\V{X}}{\V{P}}{\V{b}}} \\
\All{(\DHindx{\V{H}}{\V{D}})}{& \V{X}}{\V{P}}{\pair{\V{f}}{\V{b}}{}} &
    \TIMES{(\PI{\V{i}}{\V{H}} \V{P}\: (\V{f}\: \V{i}))}
          {\All{\V{D}}{\V{X}}{\V{P}}{\V{b}}}
\end{array}
}
&
%%
\stk{
\begin{array}{@{}ll}
\all{\!}{\!}{\!}{\!}{\!} : & \PITEL{\V{D}}{\Desc}
                             \PITEL{\V{X}}{\Set}
                             \PITEL{\V{P}}{\V{X} \To \Set} \\
                           & \PITEL{\V{R}}{\PI{\V{x}}{\V{X}} \V{P}\: \V{x}}
                             \PI{\V{xs}}{\descop{\V{D}}{\V{X}}} 
                             \All{\V{D}}{\V{X}}{\V{P}}{\V{xs}} 
\end{array} \\
\begin{array}{@{}l@{}l@{\:=\:\:}l}
\all{\DUnit}{& \V{X}}{\V{P}}{\V{R}}{\Void} &
    \Void \\
\all{(\DSigma{\V{S}}{\V{T}})}{& \V{X}}{\V{P}}{\V{R}}{\pair{\V{a}}{\V{b}}{}} &
    \all{(\V{T}\: \V{a})}{\V{X}}{\V{P}}{\V{R}}{\V{b}} \\
\all{(\DIndx{\V{D}})}{& \V{X}}{\V{P}}{\V{R}}{\pair{\V{a}}{\V{b}}{}} &
    \pair{\V{R}\: \V{a}}
         {\all{\V{D}}{\V{X}}{\V{P}}{\V{R}}{\V{b}}}{} \\
\all{(\DHindx{\V{H}}{\V{D}})}{& \V{X}}{\V{P}}{\V{R}}{\pair{\V{f}}{\V{b}}{}} &
    \pair{\LAM{\V{i}} \V{R}\: (\V{f}\: \V{i})}
         {\all{\V{D}}{\V{X}}{\V{P}}{\V{R}}{\V{b}}}{}
\end{array}
\end{array}
}
\]

\caption{Induction predicates}
\label{fig:all-predicates}

\end{figure*}


\subsection{Extending type propagation}

\begin{wstructure}
<- Extending type propagation
    <- datatype declaration turns into definitions
        -> Straightforward translation to Desc
        -> Creation of a variable referring to the structure
    <- Labelled Mu
        /> Just mention the possibility of labelling, no details required
        -> For the user, objects have names rather than Mu of codes
    <- Push Mu to an applied name [figure]
        -> Direct integration into the type propagation machinery
    -> Coded presentation is practical
        <- The user never see a code
\end{wstructure}


We have now enough machinery to build and manipulate inductive
types. In a word, we ought be able to \emph{program} over these
datatypes.  However, one could argue that programming with these
objects is not practical. 

Writing codes instead of datatype constructor is indeed unpractical.
However, regaining this ability is simply a matter of type
propagation. We extend the type-checking framework of
Figure~\ref{fig:type-checking} with the following inference rule:
%
\[
\Rule{\propag{\push{\widetilde{\V{e}}}
                   {\descop{\V{f}\: \V{C}}{(\Mu{(\DSigma{(\Enum\: \V{b})}{\V{f}})})}}}
            {\widetilde{\V{e}}'}}
     {\propag{\push{\V{C}\: \vec{\V{e}}}
                   {\Mu{(\DSigma{(\Enum\: \V{b})}{\V{f}})}}}
             {\Con{(\etag{\V{C}}, \widetilde{\V{e}}')}}}\;\etag{\V{C}} \in \V{b}
\]
%
Where $\vec{\V{e}}$ denotes a telescope of arguments applied to the
constructor $\V{C}$. For readability, we write $\V{f}\: \V{C}$ instead
of a call to $\F{switch}$. Also, we note $\widetilde{\V{e}}$ the
right-nested tuple terminated by $\Void$ corresponding to the curried
telescope $\vec{\V{e}}$.

Hence, in the high-level expression language, the developer writes a
familiar type constructor, applied to a telescope of arguments. During
type propagation, this expression will be pushed to a $\Mu{}$ type of
a tagged description. It is then straightforward to look-up the tag
and turn the constructor form into the appropriate code.

This concludes our presentation of the universe of descriptions. In
this section, we have seen how datatypes from the simply-typed world
can be expressed in a dependently-typed setting. By relying on a
universe, we benefit from the ability to inspect and manipulate
codes. We have shown that programming with codes is practical. 
