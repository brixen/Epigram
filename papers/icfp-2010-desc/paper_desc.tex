\section{A universe of simple data-types}

\begin{wstructure}
<- Why starting with simple data-types
    <- For pedagogical purposes
        <- Data-types as we know them every day
        /> Target dependent types
    -> Cut down version of Induction Recursion
        -> Presentation evolves independently as we go to dependent types
\end{wstructure}

In this section, we describe an implementation of inductive types, as
we know them in simply-typed languages such as Haskell or OCaml. This
choice is motivated by pedagogical considerations: by describing the
layman data-types in our setting, we build on our intuition of
data-types we are familiar with. However, our goal is and remains to
discuss data-types as we find them in dependently-typed languages.

Our proposal is inspired by, but less involved than, Dybjer and
Setzer's closed formulation of
induction-recursion~\cite{dybjer:general-ir, dybjer:axiom-ir,
  dybjer:ir-initial-algebra, dybjer:iir}. In the context of simple
data-types, the full power of induction-recursion is not
necessary. Therefore, we adopt a stripped-down version of it. As we
extend the scope of our universe of data-types, our presentation will
diverge from induction-recursion. We shall compare both approaches in
Section~\ref{sec:discussion}.

\subsection{The power of $\Sigma$}

\begin{wstructure}
<- The duality of Sigma
    <- Sigma generalises sum over arbitrary arities
        -> \Sigma A B == \Sigma_{x : A} B x
    <- Sigma generalises product to have a dependant second component
        -> \Sigma A B == (x : A) \times (B x)
\end{wstructure}

In dependently-typed language, $\Sigma$-types can be interpreted in
two different ways. This duality is actually reflected in the notation
we can find in the literature, depending on the sensibility of the
author. Hence, the type $\SigmaTy{A}{B}$ is sometimes written
$\Sigma_{x : A} (B\: x)$. This notation stresses the fact that
$\Sigma$-types are a generalisation of sums over arbitrary
arities. When simply-typed languages have finite sums,
dependently-typed languages have sums indexed by any set.

On the other hand, $\SigmaTy{A}{B}$ is sometimes written
$\SIGMA{\V{x}}{A} (B\: x)$. Under this perspective, $\Sigma$-types can
be read as a generalisation of products, where the second component
can depend on the first one. When simply-typed languages pack data
into non-dependent tuples, dependently-types languages tolerate this
non-dependent usage. However, they also give the rather novel ability
for data to influence further data.

\begin{wstructure}
<- Data-types in the simply-typed world
    -> "sums-of-product"
        <- Sum of constructors
        <- Product of arguments
<- Data-types in the dependently-typed world
    -> "sigmas-of-sigmas"
    /> Need ability to manipulate these sigmas
        -> Define a Code for data-types
        -> Together with a sigma-based Interpretation
\end{wstructure}

In the simply-typed world, the essence of data-type definition can be
summed up by the expression \emph{sum-of-product}. A data-type is defined
by a finite sum -- a choice -- of constructors, each of them is
composed of a product -- a tuple -- of arguments. This form governs
the grammar of data-type definition in languages such as Haskell or
OCaml.

To model these data-types, we simply need to capture this grammar in
our dependently-typed setting. Here, the notion of sum-of-product
naturally translates into \emph{sigmas-of-sigmas}. However, beyond
this intuition, it should be clear that a direct encoding of
data-types through raw $\Sigma$-types is not a viable option. Indeed,
just as for finite sets, encoding throw away information when we
crucially need it. In the realm of data-types, things become even more
tougher, as one could legitimately expect an induction principle, for
instance. Anonymous in a see of $\Sigma$s, our data-types have little
chance of survival. \note{Ridiculous tentative of poetry. I was
  really tired when I wrote that.}


\subsection{A universe of descriptions}
\label{sec:desc-universe}

\begin{wstructure}
<- Introduction to Universe construction
    <- Define a Code
        -> Name objects
    <- Define an Interpretation of codes into the type theory
        -> Give a semantics to objects
    -> Ability to manipulate code
    -> Ability to compute with these objects
\end{wstructure}

For our sigmas-of-sigmas to be known, we crucially need the ability to
name them. \note{Pursuing this crappy analogy. Solving the type
  presentation/representation debate would help solving that point.}
To this end, we will use a standard technique in dependently-typed
programming: we construct an ad-hoc universe. This technique dates
back to Martin-L\"of definition of type
theory~\cite{martin-lof:itt}. Since then, it has been fruitful as a
programming technique~\cite{benke:universe-generic-prog, whoelse?}
\note{I would need some other example of universe construction for
  practical purposes}. We refer the reader to the Agda
tutorial~\cite{norell:agda-tutorial} for a pedagogical presentation of
universe construction. \note{Is that really a motivating motivation
  for the usage of universe construction? }


The key idea behind universe construction is our ability to make names
by defining new types. These names are called \emph{codes}. By
defining a set of codes, we somehow define the syntax of a
language. However, as such, a system of code is useless as it lacks a
semantics. Instead of equipping the universe of codes with some
computational behaviour, we pragmatically chose to \emph{interpret}
these codes back into the standard type theory. Hence, codes act only
as labels, while the type theory provides the computational
machinery. \note{Is that a good description?}

Codes being simple labels, we have the ability to inspect them, hence
taking advantage of structural information. Being able to inspect
them, we are therefore free to compute over them: deriving new codes
from previous ones, or even new functions on them. In several
occasions, we will have the opportunity to witness the power of
universes.

\begin{wstructure}
<- Justification of the code 
    <- [both figures]: cannot be read separately
    <- Mimic the standard grammar of data-types description
        -> Just as we already know it
        <- '\Sigma for making sigmas-of-sigmas
        <- 'indx for exhibiting the functoriality
            -> For recursive arguments
        <- '1 for end of description
\end{wstructure}

We propose to embed inductive types as a universe in our dependent
type theory, the universe of \emph{descriptions}
(Figure~\ref{fig:desc_universe}). As expected, the code of this
universe mimics the standard grammar of data-types definitions in
simply-typed languages. Hence, we have a $\DSigma{}{}$ code,
interpreted as a $\Sigma$-type, to build the
sigmas-of-sigmas. Descriptions are terminated by $\DUnit$, which
contains no useful payload. The functoriality of the data-types is
introduced by $\DIndx$. When we tie the knot with a fix-point, the
hole left open by $\DIndx$ will be turned into a standard recursive
argument. We notice that this functoriality appears in the type of
$\descop{D}{}$ itself, for a given data-type definition $D$. This
corresponds to the object part of the functor described by $D$.

\begin{figure*}

\[
\begin{array}{ll}
\stk{
\data \Desc : \Set \where \\
\;\;\begin{array}{@{}l@{\::\:\:}l@{\quad}l}
    \DUnit          & \Desc \\
    \DSigma         & \PI{\V{S}}{\Set} \PIS{S \To \Desc} \Desc \\
    \DIndx          & \Desc \To \Desc
\end{array}
}
&
\stk{
\descop{\_\:}{} : \Desc \To \Set \To \Set \\
\begin{array}{@{}ll@{\:=\:\:}ll}
\descop{\DUnit}{& X}        &  \Unit                                       \\
\descop{\DSigma{S}{D}}{& X} &  \SIGMAS{\V{s} : S}{\descop{D~s}{X}}         \\
\descop{\DIndx{D}}{& X}     &  \TIMES{X}{\descop{D}{X}}
\end{array}
}
\end{array}
\]


\caption{Universe of Descriptions}
\label{fig:desc_universe}

\end{figure*}

To give some intuition on this universe of descriptions, we now turn
to some examples. For obvious pedagogical reasons, we will manually
build these descriptions. However, it should be clear that, in
practice, these definitions can be automatically constructed from an
Haskell-like $\data$ definition.

\subsection{Examples}
\label{sec:desc-examples}

\begin{wstructure}
<- Nat
    <- Sum of zero, suc
    <- zero case: done
    <- suc case: leave open and done
    -> NatD Z = 1 + Z
\end{wstructure}

Our first example is the natural numbers, or rather its signature
functor. Our code is presented in the high-level expression language
of Section~\ref{sec:type-propagation}. The translation back to the raw
terms is laborious but should not pose any difficulty. The code is the
following:

\[\stk{
\NatD : \Desc \\
\NatD \mapsto \DSigma{(\EnumT{[ \NatZero, \NatSuc{} ]})}
                     {[ \DUnit \quad (\DIndx{\DUnit}) ]}
}\]

Let us explain its construction. First, we use $\DSigma{}{}$ to give a
choice between the $\NatZero$ and $\NatSuc{}$ constructors. In the
$\NatZero$ case, we reach the end of the description: there is no
useful payload in that case. In the $\NatSuc{}$ case, we left one hole
open for the recursive argument, and we close the description.

In a more synthetic notation, we have described the following functor:

\[    \NatD\: Z \mapsto 1 + Z    \]



\begin{wstructure}
<- List
    <- Sum of nil, cons
    <- nil case: done
    <- cons case: product of X with leave open and done
    -> ListD X Z = 1 + X * Z
\end{wstructure}

With a small change to the definition of $\NatD$, we obtain the
signature of lists:

\[\stk{
\ListD : \Set \To \Desc \\
\ListD \: X \mapsto \DSigma{(\EnumT{[ \ListNil, \ListCons ]})}
                           {[ \DUnit \quad (\DSigma{X}{\LAM{\_} \DIndx{\DUnit}}) ]}
}\]

The modification consists in turning the $\NatSuc{}$ constructor into
a proper $\ListCons$ taking an argument in X followed by a recursive
argument. In this case, we use $\DSigma{}{}$ in its product
interpretation: we pack an element of $X$ together with the recursive
argument. Easily, one sees that this code actually describes the
following functor:

\[    \ListD\: X\: Z \mapsto 1 + X \times Z     \]

\begin{wstructure}
<- Tree
    <- sum of leaf, node
    <- leaf case: done
    <- node case: product of X with two leave open and done
    -> TreeD X Z = 1 + X * Z * Z
\end{wstructure}

Finally, we are not limited to one recursive argument. This is
demonstrated by our description of binary trees:

\[\stk{
\TreeD : \Set \To \Desc \\
\begin{array}{@{}ll}
\TreeD \: X \mapsto \DSigma{ & (\EnumT{[ \TreeLeaf, \TreeNode ]}) \\}
                           { & [ \DUnit \quad (\DSigma{X}{\LAM{\_} \DIndx{(\DIndx{\DUnit})}}) ]}
\end{array}
}\]

Again, we are at one evolutionary step away from $\ListD$. However,
instead of a single call to the induction code, we call it twice. The
interpretation of this code corresponds to the following functor:

\[    \TreeD\: X\: Z \mapsto 1 + X \times Z \times Z     \]


\begin{wstructure}
<- Tagged description
    <- Form TDesc = List (UId x Desc) [equation]
    <- Follow usual sums-of-product presentation of data-type
        <- Finite set of constructors
        <- Then whatever you want
    -> Any Desc data-type can be turned into this form
        -> No loss of expressive power
        /> Guarantee a ``constructor form''
\end{wstructure}

From the examples above, we observe that data-types are defined by a
$\DSigma$ which first argument is a finite set of constructors. The
descriptions fitting into this pattern are called \emph{tagged}
description. Formally, we have:
\note{We have talked about a tagged description based on $\List (\UId
  \times \Desc)$. It makes the free monad easier to construct but,
  imho, considerably clutters anything else. I've reverted back to a
  more sigma-friendly presentation.}


\[
 \TagDesc \mapsto \SIGMA{\V{E}}{\EnumU} (\spi{\V{E}}{(\LAM{\_} \Desc)})
\]

Again, this is a clear reminiscence of the sum-of-product style:
we have a finite sum of constructors. Of course, every description can
be expressed in this style, using a singleton as single constructor if
necessary. A tagged description can trivially be turned into a
full-blown description. We note $\toDesc{TD}$ the description computed
from the tagged description $TD$.

\begin{wstructure}
<- Fictive object [figure 'data Desc']
    -> Must be read as a type signature
    -> See further for its actual implementation
        <- Subject to our levitation exercise
\end{wstructure}

So far, for convenience, we have taken as granted the existence of
$\Desc$, as presented in Figure~\ref{fig:desc_universe}. In
particular, we have considered its code, $\DSigma{}{}$, $\DIndx{}$,
and $\DUnit$ as type formers, extending the basic type theory. Hence,
in an implementation of this type theory, we need to extended it with
these constructors and their typing rules. Although it makes no
conceptual difference, the code of description should rather be read
as a specification. We promise the existence of such objects,
satisfying the typing rules. It will be the subject of
Section~\ref{sec:desc-levitate} to fulfil this promise, by actually
implementing the specification.

\subsection{Fix-point}
\label{sec:desc-fix-point}

\begin{wstructure}
<- Build the fix-point of functors
    <- See examples: need to build their initial algebra
    -> Extend the type theory with Mu/Con [figure]
        <- Straightforward definition of a fix-point
            <- Interpret D with (Mu D) as sub-objects
\end{wstructure}

\note{ Strictly positive types anyone? }

So far, we have used our universe of descriptions to build
functors. To illustrate its usage, we have implemented the signature
functors of natural numbers, lists, and binary trees. However, the
class of functors expressible with descriptions enjoy another
property: they all admit a fix-point. Indeed, descriptions are
interpreted into \emph{polynomial functors}~\cite{who?}\note{who is
  the relevant source for polynomial functors?}, as they are only
composed of $\Sigma$-types, the unit type, and variables.

We construct the fix-point by \emph{tying the knot} of the
interpretation: the holes left open by the functor are filled by its
own recursive definition:

\[
\Rule{\Gamma \vdash \Bhab{D}{\Desc}}
     {\Gamma \vdash \Bhab{\Mu{D}}{\Set}} \qquad
\Rule{\Gamma \vdash \Bhab{D}{\Desc} \quad 
      \Gamma \vdash \Bhab{x}{\descop{D}{(\Mu{D})}}}
     {\Gamma \vdash \Bhab{\Con{x}}{\Mu{D}}}
\]

At this stage, we have recovered the data-types we are usually able to
define in simply-typed languages.


\begin{wstructure}
<- Elimination on Mu
    <- We are used to foldD : \forall X. (desc D X -> X) -> mu D -> X
        /> Not dependent
        -> Cannot express some (which one again?) properties
    -> Develop a dependent induction
        <- Everywhere/All
        <- Induction
    -> *Generic*
    ???
\end{wstructure}

Together with the type formers defined above, we would legibly expect
an elimination principle. Following a categorical intuition, we could
be tempted to provide it as a catamorphism:

\[
\F{cata} : \PITEL{D}{\Desc}
           \PI{T}{\Set}
           (\descop{D}{T} \To T) \To 
           \Mu{D} \To T 
\]

Whereas this definition is amply sufficient in the simply-typed world,
it comes short of its promises in the dependently-typed realm. Indeed,
this type is not dependent. As this operator will be the corner stone
of any inductive definition, we are better off sharping it before use.

The first step consists in turning $T$ into a predicate
$\Bhab{P}{\Mu{D} \To \Set}$, hence introducing a dependency on the
descriptions. Consequently, we strengthen the algebra $\descop{D}{T}
\To T$ by a similar process. We start with turning the arrow into a
dependent arrow, hence offering $\Bhab{xs}{\descop{D}{(\Mu{D})}}$ to
further analysis. 

Then, we have to translate the notion of algebra in our dependent
setting. This is achieved by the predicate $\All{D}{X}{P}{x}$
(Fig.~\ref{fig:all-predicates}) that, intuitively, states that
$\Bhab{P}{X \To \Set}$ holds everywhere in $x$. Based on that
knowledge, we ought to be able to prove that $P$ holds for
$\Con{xs}$. We close our elimination principle by taking an argument
$x$ in $\Mu{D}$ and claiming that the motive $P$ is realised on $x$.

\[
\begin{array}{lcll}
\F{induction} & : & \multicolumn{2}{l}{\PITEL{D}{\Desc}}                   \\
              &   & \multicolumn{2}{l}{\PITEL{P}{\Mu{D} \To \Set}}         \\
              &   & \PITEL{m}{& \PI{xs}{\descop{D}{(\Mu{D})}}              \\
              &   &           & \All{D}{(\Mu{D})}{P}{xs} \To P (\Con{xs})} \\
              &   & \multicolumn{2}{l}{\PI{x}{\Mu{D}} P x}
\end{array}
\]

\begin{figure*}

\[
\begin{array}{ll}
%%
\stk{
\begin{array}{@{}ll}
\All{}{}{}{} : & \PITEL{\V{D}}{\Desc}
                 \PITEL{\V{X}}{\Set}
                 \PITEL{\V{P}}{\V{X} \To \Set} \\
               & \PI{\V{xs}}{\descop{D}{\Mu{X}}} 
                 \Set 
\end{array} \\
\All{\DId}{X}{P}{x} \mapsto P\: x \\
\All{(\DConst{Z})}{X}{P}{x} \mapsto Z \\
\All{(\DProd{D}{D'})}{X}{P}{\pair{d}{d'}{}} \mapsto \TIMES{\All{D}{X}{P}{d}}{\All{D'}{X}{P}{d'}} \\
\All{(\DSigma{S}{T})}{X}{P}{\pair{a}{b}{}} \mapsto \All{(T\: a)}{X}{P}{b} \\
\All{(\DPi{S}{T})}{X}{P}{f} \mapsto \PI{\V{s}}{S} \All{(T\: a)}{X}{P}{(f\: s)}
}
&
%%
\stk{
\begin{array}{@{}ll}
\all{}{}{}{}{} : & \PITEL{\V{D}}{\Desc}
                   \PITEL{\V{X}}{\Set}
                   \PITEL{\V{P}}{\V{X} \To \Set} \\
                 & \PITEL{\V{R}}{\PI{\V{x}}{X} P\: x}
                   \PI{\V{xs}}{\descop{D}{\Mu{X}}} 
                   \All{D}{X}{P}{xs} 
\end{array} \\
\all{\DId}{X}{P}{R}{x} = R x \\
\all{(\DConst{Z})}{X}{P}{R}{x} = z \\
\all{(\DProd{D}{D'})}{X}{P}{R}{\pair{d}{d'}{}} = \pair{\all{D}{X}{P}{R}{d}}{\all{D'}{X}{P}{R}{d'}}{} \\
\all{(\DSigma{S}{T})}{X}{P}{R}{\pair{a}{b}{}} = \all{(T\: a)}{X}{P}{R}{b} \\
\all{(\DPi{S}{T})}{X}{P}{R}{f} = \LAM{s} \all{(T\: s)}{X}{P}{R}{(f\: s)}
\end{array}
}
\]

\caption{Induction predicates}
\label{fig:all-predicates}

\end{figure*}

At this stage, we are still left with implementing this
$\F{induction}$ principle. For space reason, we refer the reader to
our technical report for the actual code. Having this dependent
catamorphism, we are empowered with the ability to $\emph{compute}$
over descriptions. Note that this induction principle is the first
manifestation of a generic operation over descriptions. Thanks to
$\F{induction}$, any data-type we define automatically comes equipped
with an induction principle.


\subsection{Extending type propagation}

\begin{wstructure}
<- Extending type propagation
    <- Data-type declaration turns into definitions
        -> Straightforward translation to Desc
        -> Creation of a variable referring to the structure
    <- Labelled Mu
        /> Just mention the possibility of labelling, no details required
        -> For the user, objects have names rather than Mu of codes
    <- Push Mu to an applied name [figure]
        -> Direct integration into the type propagation machinery
    -> Coded presentation is practical
        <- The user never see a code
\end{wstructure}


So far, we have introduced a universe of description. We have shown
how to manipulate these codes to build data-types. We have also
presented a fix-point construction and its associated dependent
catamorphism. However, one could argue that programming with these
objects is not practical.

Whereas simply-typed languages feature a convenient grammar for
defining data-types and manipulating them, our proposal could be seen
as a step backward. In our setting, the user would have to write a
description instead of a sugared data-type definition. Then, to
recognise an element of $\Nat$ for instance, she would have to
identify $\Mu{}$ of a description as indeed the fix-point of the
functor signature of natural numbers. Finally, she would have to write
codes instead of the natural data-type constructor.

As for finite sets previously, these symptoms clearly indicate that we
suffer from too much type information. Just as for finite sets, the
cure lies in rationalising this information. Let us first tackle the
issue of data-type declaration. The grammar of data-types we are used
to can be straightforwardly translated to the sigmas-of-sigmas
paradigm. Hence, a data-type definition can be turned into the
corresponding description. Then, at the level of definitions, we
associate the name of the data-type to the fix-point of the
description. The induction principle as well as other generic
operations follow for free.

Doing so, we seal data-type definition behind a convenient
abstraction. We take advantage of this abstraction barrier to tamper
the second issue. Namely, our user would rather be presented a name
rather than a fix-point of a signature functor. Surprisingly, it is
sufficient to \emph{label} the $\Mu{}$ constructor with the
user-provided name. When pretty-printing this object, we just expose
the label, instead of the definition in full.

Regaining the ability to write data-type constructor is simply a
matter of type propagation. We therefore extends the type-checking
framework of Figure~\ref{fig:type-checking} with the following
inference rule:

\note{ This is rather drafty. I should explain what $\vec{e}$ and $\widetilde{e'}$ are. }

\[
\Rule{\forall e_i \in \vec{e}, \propag{\push{e_i}
                                            {\descop{f\: i}{(\Mu{(\DSigma{(\Enum\: b)}{f})})}}}
                                      {e'_i}}
     {\propag{\push{C\: \vec{e}}{\Mu{(\DSigma{(\Enum\: b)}{f})}}}
             {\Con{(\etag{C}, \widetilde{e'})}}}\;\etag{C} \in b
\]

Hence, in the high-level expression language, the user types a
familiar type constructors, applied to a telescope of arguments. In
the type propagation setting, this expression will be pushed to a
$\Mu{}$ type of a tagged description. It is then straightforward to
look-up the tag and turn the constructor form into the appropriate
code.

This concludes our presentation of the universe of descriptions. In
this section, we have seen how data-types from the simply-typed world
can be expressed in our dependently-typed setting. By relying on a
universe construction technique, we benefit from the ability to
inspect and manipulate these codes. This opens some appealing
opportunities for generic programming. Finally, we have demonstrated
that programming with codes is practical. To this end, we have
extended the type propagation machinery. In this setting, we can
define, interact, and build data-types as transparently as in the
simply-typed world.
