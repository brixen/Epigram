\section{A universe of simple data-types}

\begin{wstructure}
<- Why starting with simple data-types
    <- For pedagogical purposes
        <- Data-types as we know them every day
        /> Target dependent types
    -> Cut down version of Induction Recursion
        -> Presentation evolves independently as we go to dependent types
\end{wstructure}

In this section, we describe an alternative implementation of
inductive types, as we know them in simply-typed languages such as
Haskell or OCaml. This choice is motivated by pedagogical
considerations: by describing the layman data-types in our setting, we
build on our intuition of data-types we are familiar with. However,
our goal is and remains to discuss data-types as we find them in
dependently-typed languages.

Our proposal is based on induction-recursion~\cite{dybjer:general-ir,
  dybjer:axiom-ir, dybjer:ir-initial-algebra, dybjer:iir}. However, in
the context of simple data-types, the full power of
induction-recursion is not necessary. Therefore, we will present a
stripped-down version. As we extend the scope of our universe of
data-types, our presentation will diverge from induction-recursion. We
shall compare both approaches in Section~\ref{sec:discussion}

\subsection{The power of $\Sigma$}

\begin{wstructure}
<- The duality of Sigma
    <- Sigma generalises sum over arbitrary arities
        -> \Sigma A B == \Sigma_{x : A} B x
    <- Sigma generalises product to have a dependant second component
        -> \Sigma A B == (x : A) \times (B x)
\end{wstructure}

In dependently-typed language, $\Sigma$-types can be read in two
rather different ways. This duality is actually reflected in the
notation we can find in the literature, depending on the sensibility
of the author. Hence, the type $\SigmaTy{A}{B}$ is sometimes written
$\Sigma_{x : A} (B x)$. This notation stresses the fact that
$\Sigma$-types are a generalisation of sums over arbitrary
arities. When simply-typed languages have finite sums,
dependently-typed languages have sums of indexed by any set.

On the other hand, $\SigmaTy{A}{B}$ is sometimes written
$\SIGMA{\V{x}}{A} (B x)$. Under this perspective, $\Sigma$-types can
be read as a generalisation of products, where the second component
can depend on the first one. When simply-typed languages pack data
into non-dependent tuples, dependently-types languages tolerate this
non-dependent usage. However, they also give the rather novel ability
for data to influence further data.

\begin{wstructure}
<- Data-types in the simply-typed world
    -> "sums-of-product"
        <- Sum of constructors
        <- Product of arguments
<- Data-types in the dependently-typed world
    -> "sigmas-of-sigmas"
    /> Need ability to manipulate these sigmas
        -> Define a Code for data-types
        -> Together with a sigma-based Interpretation
\end{wstructure}

In the simply-typed world, the essence of data-type definition can be
summed up by the term ``sum-of-product''. A data-type is defined by a
finite sum, choice, of constructors, each of them composed by a
product, a tuple, of arguments. Therefore, the grammar of data-types
definition in simply-typed languages, such as Haskell or OCaml, follow
this form.

To model these data-types, we are simply left with capturing this
grammar in a dependently-typed setting. Hence, the notion of
``sum-of-product'' naturally translates into
\emph{sigmas-of-sigmas}. However, beyond this intuition, it should be
clear that a direct encoding of data-types through raw $\Sigma$-types
is not a viable option. Indeed, just as for finite sets, encoding
throw away information when we crucially need it. In the realm of
data-types, things become even more tougher, as one could legitimately
expect an induction principle, for instance. Anonymous in a see of
$\Sigma$s, our data-types have little chance of survival.


\subsection{A universe of descriptions}

\begin{wstructure}
<- Introduction to Universe construction
    <- Define a Code
        -> Name objects
    <- Define an Interpretation of codes into the type theory
        -> Give a semantics to objects
    -> Ability to manipulate code
    -> Ability to compute with these objects
\end{wstructure}

\note{Check Martin-Lof reference}

For our sigma-of-sigmas to be known, we crucially need the ability to
name them. To this end, we will use a standard technique in
dependently-typed programming: we construct an ad-hoc universe. This
technique dates back to Martin-L\"of definition of type
theory~\cite{martin-lof:itt}. Since then, it has been fruitful as a
programming technique~\cite{who?} \note{I would need some other
  example of universe construction for practical purposes}. We refer
the reader to Agda's tutorial~\cite{norell:agda-tutorial} for a
pedagogical presentation of universe construction.

The key idea behind universe construction is our ability to make names
by defining new types. These names are called \emph{codes}. By
defining a set of codes, we somehow define the syntax of a
language. However, as such, a system of code is useless as it lacks a
semantics. Instead of equipping the universe of codes with some
computational behaviour, we pragmatically chose to \emph{interpret}
these codes back into the standard type theory. Hence, codes act only
as labels, while the type theory provides the computational machinery.

Codes being simple labels, we have the ability to inspect them, hence
taking advantage of their structural information. Being able to
inspect them, we are therefore free to compute over them: deriving new
codes from previous ones, or even new functions on them. In several
occasions, we will have the opportunity to witness the power of
universes.

\note{Is that really a motivating motivation for the usage of universe
  construction? }

\begin{wstructure}
<- Justification of the code 
    <- [both figures]: cannot be read separately
    <- Mimic the standard grammar of data-types description
        -> Just as we already know it
        <- '\Sigma for making sigmas-of-sigmas
        <- 'indx for exhibiting the functoriality
            -> For recursive arguments
        <- '1 for end of description
\end{wstructure}

Hence, we propose to embed inductive types as a universe in our
dependent type theory, the universe of \emph{descriptions}
(Figure~\ref{fig:desc_universe}). As expected, the code of this
universe mimics the standard grammar of data-types definitions in
simply-typed languages. Hence, we have a $\DSigma{}{}$ code,
interpreted as a $\Sigma$-type, to build the
sigmas-of-sigmas. Descriptions are terminated by $\DUnit$, which
contains no useful payload. The functoriality of the data-types is
introduced by $\DIndx$. When we tie the knot with a fix-point, the
hole left open by $\DIndx$ will be turned into a standard recursive
argument. We notice that this functoriality appears in the type of
$\descop{D}{}$ itself, for a given data-type definition $D$. This
corresponds to the morphism part of the functor described by $D$.

\begin{figure*}

\[
\begin{array}{ll}
\stk{
\data \Desc : \Set \where \\
\;\;\begin{array}{@{}l@{\::\:}l@{\quad}l}
    \DUnit          & \Desc \\
    \DSigma         & \PI{\V{S}}{\Set} \PIS{S \To \Desc} \Desc \\
    \DIndx          & \Desc \To \Desc
\end{array}
}
&
\stk{
\descop{\_\:}{} : \Desc \To \Set \To \Set \\
\begin{array}{@{}ll@{\:=\:\:}ll}
\descop{\DUnit}{& X}        &  \Unit                                       \\
\descop{\DSigma{S}{D}}{& X} &  \SIGMAS{\V{s} : S}{\descop{D~s}{X}}         \\
\descop{\DIndx{D}}{& X}     &  \TIMES{X}{\descop{D}{X}}
\end{array}
}
\end{array}
\]


\caption{Universe of Descriptions}
\label{fig:desc_universe}

\end{figure*}

To give some intuition on this universe of descriptions, we now turn
to some examples. For obvious pedagogical reasons, we will manually
build these descriptions. However, it should be clear that, in
practice, these definitions can be automatically constructed an
Haskell-like $\data$ definition.

\subsection{Examples}
\label{sec:desc-examples}

\begin{wstructure}
<- Nat
    <- Sum of zero, suc
    <- zero case: done
    <- suc case: leave open and done
    -> NatD Z = 1 + Z
\end{wstructure}

Our first example is the natural numbers, or rather its carrier
functor. Our code is presented in the high-level expression language
of Section~\ref{sec:type-propagation}. The translation back to the raw
terms is laborious but should not pose any difficulty. The code is the
following:

\[\stk{
\NatD : \Desc \\
\NatD \mapsto \DSigma{(\EnumT [ \NatZero, \NatSuc{} ])}
                     {[ \DUnit \quad (\DIndx{\DUnit}) ]}
}\]

Let us explain its construction. First, we use $\DSigma{}{}$ to build
a sum between $\NatZero$ and $\NatSuc{}$. In the $\NatZero$ case, we
reach the end of the description: there is no useful payload in that
case. In the $\NatSuc{}$ case, we left one hole open for the recursive
argument, and we close the description.

In a more synthetic notation, we have simply implemented the following
functor:

\[    \NatD\: Z = 1 + Z    \]



\begin{wstructure}
<- List
    <- Sum of nil, cons
    <- nil case: done
    <- cons case: product of X with leave open and done
    -> ListD X Z = 1 + X * Z
\end{wstructure}

With a small change to the definition of $\NatD$, we obtain the
carrier of lists:

\[\stk{
\ListD : \Set \To \Desc \\
\ListD \: X \mapsto \DSigma{(\EnumT [ \ListNil, \ListCons ])}
                           {[ \DUnit \quad (\DSigma{X}{\LAM{\_} \DIndx{\DUnit}}) ]}
}\]

The modification consists in turning the $\NatSuc{}$ into a proper
$\ListCons$ taking an argument in X followed by an inductive
argument. In this case, we use $\DSigma{}{}$ in its product
interpretation: we pack an element of $X$ together with the recursive
argument. Easily, one sees that this code actually defines the
following functor:

\[    \ListD\: X Z = 1 + X \times Z     \]

\begin{wstructure}
<- Tree
    <- sum of leaf, node
    <- leaf case: done
    <- node case: product of X with two leave open and done
    -> TreeD X Z = 1 + X * Z * Z
\end{wstructure}

Finally, we are not limited to one recursive argument. This is
demonstrated by our description of a binary tree functor below:

\[\stk{
\TreeD : \Set \To \Desc \\
\begin{array}{@{}ll}
\TreeD \: X \mapsto \DSigma{ & (\EnumT [ \TreeLeaf, \TreeNode ]) \\}
                           { & [ \DUnit \quad (\DSigma{X}{\LAM{\_} \DIndx{(\DIndx{\DUnit})}}) ]}
\end{array}
}\]

Again, we are at one evolutionary step away from $\ListD$. However,
instead of single appeal to the induction code, we call it twice. The
interpretation of this code corresponds to the following functor:

\[    \TreeD\: X Z = 1 + X \times Z \times Z     \]


\begin{wstructure}
<- Tagged description
    <- Form TDesc = List (UId x Desc) [equation]
    <- Follow usual sums-of-product presentation of data-type
        <- Finite set of constructors
        <- Then whatever you want
    -> Any Desc data-type can be turned into this form
        -> No loss of expressive power
        /> Guarantee a ``constructor form''
\end{wstructure}

From the examples above, we observe that data-types are defined by a
$\DSigma$ which first argument is a finite set of constructor. The
descriptions fitting into this pattern are called \emph{tagged}
description. Formally, we have:

\note{We have talked about a tagged description based on $\List (\UId
  \times \Desc)$. It makes the free monad easier to construct but,
  imho, considerably clutters anything else. I've reverted back to a
  more sigma-friendly presentation.}


\[
 \TagDesc \mapsto \SIGMA{\V{E}}{\EnumU} \spi{\V{E}}{\LAM{\_} \Desc}
\]

Again, this is a clear reminiscence of the ``sum-of-products'' style:
we have a finite sum of constructors. Of course, every description can
be expressed into this style, by using a singleton as single
constructor.

\note{I think we need a function mapping a $\TagDesc$ to the
  corresponding $\Desc$. This function should go here.}

\begin{wstructure}
<- Fictive object [figure 'data Desc']
    -> Must be read as a type signature
    -> See further for its actual implementation
        <- Subject to our levitation exercise
\end{wstructure}

So far, for convenience, we have taken as granted the existence of
$\Desc$, as presented in Figure~\ref{fig:desc_universe}. In
particular, we have considered its code, $\DSigma{}{}$, $\DIndx{}$,
and $\DUnit$ as type formers, extending the basic type theory. Hence,
in an implementation of this type theory, we would have extended it
with these constructors and their typing rules. Although it makes no
conceptual difference, the code of description should rather be read
as a specification. We promise the existence of such objects,
satisfying the typing rules. It will be the subject of
Section~\ref{sec:desc-levitate} to fulfil this promise, by actually
implementing the specification.

\subsection{Fix-point}
\label{sec:desc-fix-point}

\begin{wstructure}
<- Build the fix-point of functors
    <- See examples: need to build their initial algebra
    -> Extend the type theory with Mu/Con [figure]
        <- Straightforward definition of a fix-point
            <- Interpret D with (Mu D) as sub-objects
\end{wstructure}

\note{ Strictly positive types anyone? }

So far, we have used our universe of descriptions to build
functors. To illustrate its usage, we have implemented the signature
functors of natural numbers, lists, and binary trees. However, the
class of functors expressible with descriptions enjoy another
property: they all admit a fix-point.

\note{ I'm afraid that this section is killed by my categorical gibberish. }

Let us constructively prove this. First of all, for an object $D$
describing a signature functor, we define its fix-point by the type
former $\Mu{D}$. Inhabitants of the fix-point are materialised by the
structure map: $\Con{x}$ inhabits $\Mu{D}$ if $x$ belongs to the
interpretation of $D$ with the carrier of the initial algebra:

\[
\Rule{\Gamma \vdash \Bhab{D}{\Desc}}
     {\Gamma \vdash \Bhab{\Mu{D}}{\Set}} \qquad
\Rule{\Gamma \vdash \Bhab{D}{\Desc} \quad 
      \Gamma \vdash \Bhab{x}{\descop{D}{(\Mu{D})}}}
     {\Gamma \vdash \Bhab{\Con{x}}{\Mu{D}}}
\]

For the less categorically inclined reader, we can simply read this
construction as \emph{tying the knot}: the holes left open by the
functor are filled by its own recursive definition. Hence, we have
recovered the data-types we are usually able to define in simply-typed
languages.

\begin{wstructure}
<- Elimination on Mu
    <- We are used to foldD : \forall X. (desc D X -> X) -> mu D -> X
        /> Not dependent
        -> Cannot express some (which one again?) properties
    -> Develop a dependent induction
        <- Everywhere/All
        <- Induction
    -> *Generic*
    ???
\end{wstructure}

Together with the type formers defined above, we would legibly expect
an elimination principle. Following the categorical intuition, we
could be tempted to provide it as a catamorphism:

\[
\F{cata} : \PITEL{D}{\Desc}
           \PI{T}{\Set}
           (\descop{D}{T} \To T) \To 
           \Mu{D} \To T 
\]

Whereas this definition is amply sufficient in the simply-typed world,
it comes short of its promises in the dependently-typed realm. Indeed,
this type is absolutely non dependent. As this operator will be the
corner stone of any inductive definition, we are better off sharping
it before use.

The first step consists in turning $T$ into a predicate
$\Bhab{P}{\Mu{D} \To \Set}$, hence introducing a dependency on the
descriptions. Consequently, we strengthen the algebra $\descop{D}{T}
\To T$ by a similar process. We start with turning the arrow into a
dependently arrow, hence offering $\Bhab{xs}{\descop{D}{(\Mu{D})}}$ to
further analysis. 

Then, we have to translate the notion of algebra in our dependent
setting. This is achieved by the $\All{D}{X}{P}{x}$ predicate that,
intuitively, states that $\Bhab{P}{X \To \Set}$ holds everywhere in
the sub-structures of $x$. Based on that knowledge, we ought to be
able to prove that $P$ holds for $\Con{xs}$. We close our elimination
principle by taking an argument $x$ in $\Mu{D}$ and claiming that the
motive $P$ is realised on $x$.

\[
\begin{array}{lcll}
\F{induction} & : & \multicolumn{2}{l}{\PITEL{D}{\Desc}}                   \\
              &   & \multicolumn{2}{l}{\PITEL{P}{\Mu{D} \To \Set}}         \\
              &   & \PITEL{m}{& \PI{xs}{\descop{D}{(\Mu{D})}}              \\
              &   &           & \All{D}{(\Mu{D})}{P}{xs} \To P (\Con{xs})} \\
              &   & \multicolumn{2}{l}{\PI{x}{\Mu{D}} P x}
\end{array}
\]

At this stage, we are still left with implementing $\F{All}$ and the
$\F{induction}$ principle. For space reason, we refer the reader to
our technical report for the actual code. Having this dependent
catamorphism, we are empowered with the ability to $\emph{compute}$
over descriptions. Note that this induction principle is the first
manifestation of a generic operation over descriptions. Thanks to
$\F{induction}$, any data-type we define automatically comes equipped
with an induction principle.

\note{Shall we give the code of $\All$?}

\begin{wstructure}
<- Extending type propagation
    <- Data-type declaration turns into definitions
        -> Straightforward translation to Desc
        -> Creation of a variable referring to the structure
    <- Labelled Mu
        /> Just mention the possibility of labelling, no details required
        -> For the user, objects have names rather than Mu of codes
    <- Push Mu to an applied name [figure]
        -> Direct integration into the type propagation machinery
    -> Coded presentation is practical
        <- The user never see a code
\end{wstructure}


In this section, we have introduced a universe of description. We have
shown how to manipulate these codes to build data-types. We have also
presented a fix-point construction and its associated dependent
catamorphism. However, one could argue that programming with these
objects is not practical. 

Whereas simply-typed languages feature a convenient grammar for
defining data-types and manipulating them, our proposal could be seen
as a step backward. In our setting, the user would have to write a
description instead of a sugared data-type definition. Then, to
recognise an element of $\Nat$ for instance, she would have to
identify $\Mu{}$ of a description as indeed the fix-point of the
functor signature of natural numbers. Finally, she would have to write
codes instead of the natural data-type constructor.

As for finite sets previously, these symptoms clearly indicate that we
suffer from too much type information. Just as for finite, the cure
lies in rationalising this information. Let us first tackle the issue
of data-type declaration: unsurprisingly, the grammar of data-types we
are used can be straightforwardly translated to the
``sigmas-of-sigmas'' paradigm. Hence, a data-type definition is turned
into the corresponding description. Then, at the level of definitions,
we associate the name of the data-type to the fix-point of the
description. The induction principle as well as other generic
operations follow for free.

Doing so, we seal data-type definition behind a convenient
abstraction. We take advantage of this abstraction barrier to tamper
the second issue. Namely, our user would rather be presented a name
rather that a fix-point of a signature functor. Rather surprisingly,
it is sufficient to \emph{label} the $\Mu{}$ constructor with the
user-provided name. When pretty-printing this object, we just expose
the label, instead of the definition in full.

Regaining the ability to write data-type constructor is simply a
matter of type propagation. We therefore extends the type-checking
framework of Figure~\ref{fig:type-checking} with the following
inference rule:

\note{ This is rather drafty. I should explain what $\vec{e}$ and $\widetilde{e'}$ are. }

\[
\Rule{\forall e_i \in \vec{e}, \propag{\push{e_i}
                                            {\descop{f\: i}{(\Mu{(\DSigma{(\Enum\: b)}{f})})}}}
                                      {e'_i}}
     {\propag{\push{C\: \vec{e}}{\Mu{(\DSigma{(\Enum\: b)}{f})}}}
             {\Con{(\etag{C}, \widetilde{e'})}}}\;\etag{C} \in b
\]

Hence, in the high-level expression language, the user types a
familiar type constructors, applied to a telescope of arguments. In
the type propagation setting, this expression will be pushed to a
$\Mu{}$ type of a tagged description. It is then straightforward to
look-up the tag and turn the constructor form into the appropriate
code.

This concludes our presentation of the universe of descriptions. In
this section, we have seen how data-types from the simply-typed world
can be expressed in our dependent-type setting. By relying on a
universe construction technique, we benefit from the ability to
inspect and manipulate these codes. This opens some appealing
opportunities for generic programming. Finally, we have demonstrated
that programming with codes is practical. To this end, we have
extended the type propagation machinery. In this setting, we can
define, interact, and build data-types as transparently as in the
simply-typed world.
